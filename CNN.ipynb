{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#A Convolutional Network implementation example using TensorFlow library.\n",
    "#This example is using the MNIST database of handwritten digits\n",
    "#(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "#Author: Aymeric Damien\n",
    "#Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Based on above project, modified by James Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import loader #loader for mnist dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pdb, time, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist dataset\n",
    "\n",
    "The MNIST database of handwritten digits. [[website]](http://yann.lecun.com/exdb/mnist/)<br>\n",
    "There are **60,000** training images and **10,000** testing images in this dataset.<br>\n",
    "Each digit is a one-channel image. Size of image = 28*28 = 784.\n",
    "\n",
    "![](imgs/mnist_ex.png)\n",
    "\n",
    "There are some build-in mnist function can be used in tensorflow.\n",
    "\n",
    "Ex.<br>\n",
    "from tensorflow.examples.tutorials.mnist import input_data<br>\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "Instead of using these functions, I'll use the orginal dataset manually in this code.<br>\n",
    "It's more clear to trace the data-processing.\n",
    "\n",
    "When we load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load mnist data manually\n",
    "# loading 'train' or 'test' data\n",
    "# ex. load_mnist_data('train')\n",
    "# return images, labels and mean of all images. (But, we'll only use the mean of training data.)\n",
    "# ims: [N * 784]\n",
    "# labels: [N]\n",
    "# ims_mean: [784]\n",
    "\n",
    "def load_mnist_data(flag, data_path='data'):\n",
    "    data_loader = loader.MNIST(data_path)\n",
    "    if flag == 'train':\n",
    "        ims, labels = data_loader.load_training()\n",
    "    elif flag == 'test':\n",
    "        ims, labels = data_loader.load_testing()\n",
    "    else:\n",
    "        raise ValueError(\"Error. Only training or testing data.\")\n",
    "    ims = ims/255.0\n",
    "    ims_mean = np.mean(ims, axis=0)\n",
    "    return ims, labels, ims_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001  # default = 0.001\n",
    "training_epochs = 2 # default = 2\n",
    "batch_size = 50   # training batch size, default = 50\n",
    "test_batch_size = 100\n",
    "display_step = 50  # testing, default = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "stddev=0.01    # standard deviation for random initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions of Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "\n",
    "# convolutional function\n",
    "# input: \n",
    "# x=[batch_size, height, width, channels]\n",
    "# W(Weights)=tf.Variable(shape=[kernel_size, kernel_size, input_channel, output_channel])\n",
    "# b(Biases)=tf.Variable(shape=[output_channel])\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])  # mnist input images, [batch_size x 784]\n",
    "    y = tf.placeholder(tf.int32,[None])              # label, [batch_size]\n",
    "    dropout = tf.placeholder(tf.float32)  #dropout ratio\n",
    "    \n",
    "    # Declare Variables \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], mean=0, stddev=stddev)),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], mean=0, stddev=stddev)),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024], mean=0, stddev=stddev)),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.truncated_normal([1024, n_classes], mean=0, stddev=stddev))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),  # default stddev=1.0\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1]) # Transfer shape. Prepare for convolution\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x_reshape, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob = 1-dropout)   # dropout ratio --> keep ratio\n",
    "\n",
    "    # Output, class prediction\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    #one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #one_hot_y = tf.cast(one_hot_y, tf.float32)\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=one_hot_y))\n",
    "\n",
    "    \n",
    "\n",
    "    probs = tf.nn.softmax(pred)\n",
    "    log_probs = tf.log(probs + 1e-8)\n",
    "\n",
    "    one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #print one_hot_y.get_shape()\n",
    "    #cross_entropy_loss = - tf.mul(y,log_probs)\n",
    "    cross_entropy_loss = - tf.mul(tf.cast(one_hot_y, tf.float32),log_probs)\n",
    "    \n",
    "    loss = tf.reduce_sum(cross_entropy_loss)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(one_hot_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    return x, y, dropout, loss, pred, accuracy, conv1, conv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter):\n",
    "    Train_Loss = 0\n",
    "    Test_Loss = 0\n",
    "    Train_Acc = 0\n",
    "    Test_Acc = 0\n",
    "    for idx in xrange(iter_per_epoch):\n",
    "        batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "        batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Train_Loss += C/batch_size   # calculate the loss in average (per image).\n",
    "        Train_Acc += A\n",
    "    # Eval testing dataset\n",
    "    for idx in xrange(test_iter):\n",
    "        batch_xs = ims_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]] - ims_mean\n",
    "        batch_ys = labels_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Test_Loss += C/test_batch_size\n",
    "        Test_Acc += A\n",
    "    return Train_Loss, Train_Acc, Test_Loss, Test_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_fcn(types, tag='train'):\n",
    "    # dropout=None, lr=None, batchsize=None, normalize=None\n",
    "    npzfiles=[]\n",
    "    colors = ['b-', 'r-', 'k-', 'g-', 'y-', 'c-', 'm-']\n",
    "    if len(types) > len(colors):\n",
    "        print \"only accept %d types\"%(len(color))\n",
    "        return \n",
    "    \n",
    "    for i in xrange(len(types)):\n",
    "        npz = np.load('npzfiles/'+types[i]+'.npz')\n",
    "        npzfiles.append(npz)\n",
    "    print npzfiles\n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.axis([0, training_epochs, 0, 5.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['loss'].item()['x'], npzfiles[i]['loss'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('(%)')\n",
    "    plt.axis([0, training_epochs, 0, 1.0])\n",
    "    for i in xrange(len(npzfiles)):\n",
    "        plt.plot(npzfiles[i]['acc'].item()['x'], npzfiles[i]['acc'].item()[tag+'_y'], colors[i], label=types[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------After Random Initialization------\n",
      "Training: loss=2.491240, acc=0.102183.\t\tTesting: loss=2.492461, acc=0.101000\n",
      " 5.585631 seconds\n",
      "------Start Training------\n",
      "Epoch 0.041667, Training: loss=2.322421, acc=0.097367.\t\tTesting: loss=2.323209, acc=0.098200\n",
      "Epoch 0.083333, Training: loss=2.208141, acc=0.236800.\t\tTesting: loss=2.206677, acc=0.231300\n",
      "Epoch 0.125000, Training: loss=1.113347, acc=0.682750.\t\tTesting: loss=1.099100, acc=0.689600\n",
      "Epoch 0.166667, Training: loss=0.824046, acc=0.709617.\t\tTesting: loss=0.796710, acc=0.720100\n",
      "Epoch 0.208333, Training: loss=0.640204, acc=0.789117.\t\tTesting: loss=0.621286, acc=0.801500\n",
      "Epoch 0.250000, Training: loss=0.548887, acc=0.822117.\t\tTesting: loss=0.521455, acc=0.835500\n",
      "Epoch 0.291667, Training: loss=0.442021, acc=0.857767.\t\tTesting: loss=0.422776, acc=0.868800\n",
      "Epoch 0.333333, Training: loss=0.405092, acc=0.869650.\t\tTesting: loss=0.378290, acc=0.881400\n",
      "Epoch 0.375000, Training: loss=0.375293, acc=0.880250.\t\tTesting: loss=0.353931, acc=0.889500\n",
      "Epoch 0.416667, Training: loss=0.299893, acc=0.907067.\t\tTesting: loss=0.287187, acc=0.911500\n",
      "Epoch 0.458333, Training: loss=0.309610, acc=0.900383.\t\tTesting: loss=0.294490, acc=0.906700\n",
      "Epoch 0.500000, Training: loss=0.295589, acc=0.908067.\t\tTesting: loss=0.288403, acc=0.906700\n",
      "Epoch 0.541667, Training: loss=0.281225, acc=0.909633.\t\tTesting: loss=0.263376, acc=0.911800\n",
      "Epoch 0.583333, Training: loss=0.231694, acc=0.925083.\t\tTesting: loss=0.217441, acc=0.930900\n",
      "Epoch 0.625000, Training: loss=0.244098, acc=0.922933.\t\tTesting: loss=0.227679, acc=0.928400\n",
      "Epoch 0.666667, Training: loss=0.229558, acc=0.926850.\t\tTesting: loss=0.215001, acc=0.930800\n",
      "Epoch 0.708333, Training: loss=0.240320, acc=0.920083.\t\tTesting: loss=0.232406, acc=0.919900\n",
      "Epoch 0.750000, Training: loss=0.193384, acc=0.939133.\t\tTesting: loss=0.179741, acc=0.942800\n",
      "Epoch 0.791667, Training: loss=0.216320, acc=0.930600.\t\tTesting: loss=0.203568, acc=0.932100\n",
      "Epoch 0.833333, Training: loss=0.213327, acc=0.929683.\t\tTesting: loss=0.196059, acc=0.935000\n",
      "Epoch 0.875000, Training: loss=0.219942, acc=0.928833.\t\tTesting: loss=0.203802, acc=0.933200\n",
      "Epoch 0.916667, Training: loss=0.200095, acc=0.934700.\t\tTesting: loss=0.185019, acc=0.937600\n",
      "Epoch 0.958333, Training: loss=0.175197, acc=0.944600.\t\tTesting: loss=0.167106, acc=0.945000\n",
      "Epoch 1.000000, Training: loss=0.197048, acc=0.936500.\t\tTesting: loss=0.181012, acc=0.942900\n",
      "Epoch 1, Training: loss=0.197048, acc=0.936500.\t\tTesting: loss=0.181012, acc=0.942900\n",
      "Cost 149.451492 seconds\n",
      "Epoch 1.041667, Training: loss=0.161526, acc=0.947900.\t\tTesting: loss=0.153937, acc=0.950700\n",
      "Epoch 1.083333, Training: loss=0.158858, acc=0.947767.\t\tTesting: loss=0.155123, acc=0.949100\n",
      "Epoch 1.125000, Training: loss=0.144501, acc=0.953783.\t\tTesting: loss=0.137623, acc=0.955200\n",
      "Epoch 1.166667, Training: loss=0.166041, acc=0.948033.\t\tTesting: loss=0.163924, acc=0.946800\n",
      "Epoch 1.208333, Training: loss=0.151518, acc=0.952000.\t\tTesting: loss=0.152594, acc=0.948700\n",
      "Epoch 1.250000, Training: loss=0.139866, acc=0.956317.\t\tTesting: loss=0.129189, acc=0.957700\n",
      "Epoch 1.291667, Training: loss=0.144077, acc=0.952800.\t\tTesting: loss=0.129065, acc=0.957500\n",
      "Epoch 1.333333, Training: loss=0.133662, acc=0.957083.\t\tTesting: loss=0.126176, acc=0.958500\n",
      "Epoch 1.375000, Training: loss=0.136868, acc=0.956367.\t\tTesting: loss=0.122299, acc=0.959100\n",
      "Epoch 1.416667, Training: loss=0.117119, acc=0.963183.\t\tTesting: loss=0.109745, acc=0.964700\n",
      "Epoch 1.458333, Training: loss=0.132776, acc=0.959283.\t\tTesting: loss=0.121335, acc=0.959100\n",
      "Epoch 1.500000, Training: loss=0.120812, acc=0.961950.\t\tTesting: loss=0.113993, acc=0.963600\n",
      "Epoch 1.541667, Training: loss=0.119509, acc=0.962683.\t\tTesting: loss=0.110651, acc=0.963600\n",
      "Epoch 1.583333, Training: loss=0.108531, acc=0.965133.\t\tTesting: loss=0.107522, acc=0.964900\n",
      "Epoch 1.625000, Training: loss=0.111158, acc=0.965533.\t\tTesting: loss=0.106232, acc=0.965300\n",
      "Epoch 1.666667, Training: loss=0.141684, acc=0.953733.\t\tTesting: loss=0.143905, acc=0.952400\n",
      "Epoch 1.708333, Training: loss=0.124951, acc=0.959967.\t\tTesting: loss=0.127000, acc=0.959600\n",
      "Epoch 1.750000, Training: loss=0.111899, acc=0.963600.\t\tTesting: loss=0.108642, acc=0.964000\n",
      "Epoch 1.791667, Training: loss=0.106355, acc=0.966583.\t\tTesting: loss=0.106743, acc=0.964900\n",
      "Epoch 1.833333, Training: loss=0.110718, acc=0.964967.\t\tTesting: loss=0.107828, acc=0.963300\n",
      "Epoch 1.875000, Training: loss=0.098433, acc=0.969533.\t\tTesting: loss=0.094248, acc=0.969600\n",
      "Epoch 1.916667, Training: loss=0.103084, acc=0.967700.\t\tTesting: loss=0.098696, acc=0.968200\n",
      "Epoch 1.958333, Training: loss=0.101444, acc=0.967800.\t\tTesting: loss=0.102667, acc=0.966300\n",
      "Epoch 2.000000, Training: loss=0.134914, acc=0.956450.\t\tTesting: loss=0.133648, acc=0.954500\n",
      "Epoch 2, Training: loss=0.134914, acc=0.956450.\t\tTesting: loss=0.133648, acc=0.954500\n",
      "Cost 149.315561 seconds\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data\n",
    "ims, labels, ims_mean = load_mnist_data('train', data_path='data')\n",
    "#ims_mean = np.zeros((28*28))\n",
    "ims_test, labels_test, _ = load_mnist_data('test', data_path='data')\n",
    "\n",
    "order_list = range(len(ims))\n",
    "\n",
    "# parameters related to mnist dataset \n",
    "test_iter = len(ims_test)/test_batch_size # number of testing-minibatch.\n",
    "\n",
    "iter_per_epoch = len(ims)/batch_size      # number of training-minibatch.\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    x, y, dropout, cost, pred, accuracy, conv1, conv2= lenet()\n",
    "    train_loss = cost/batch_size # loss per image\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "\n",
    "\n",
    "\n",
    "    # initialize all variables\n",
    "    try:\n",
    "        init = tf.initialize_all_variables()\n",
    "    except:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    Loss_plt = {'x':[], 'train_y':[], 'test_y':[]}\n",
    "    Acc_plt  = {'x':[], 'train_y':[], 'test_y':[]} \n",
    "    # Before Training (Random initialization), Evaluate the model one-time.\n",
    "    begin = time.time()\n",
    "    Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "    print \"------After Random Initialization------\"\n",
    "    print \"Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\" %(Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch,\n",
    "                                                                     Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "    \n",
    "    epoch = 0\n",
    "    step = 1      \n",
    "    \n",
    "    Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "    Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "    Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "    Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "    Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "    \n",
    "    duration = time.time()-begin\n",
    "    print \" %f seconds\"%(duration)\n",
    "    \n",
    "    print \"------Start Training------\"\n",
    "\n",
    "    for epoch in xrange(training_epochs):\n",
    "        begin = time.time()\n",
    "        Train_Loss = 0\n",
    "        Test_Loss = 0\n",
    "        Train_Acc = 0\n",
    "        Test_Acc = 0\n",
    "        for idx in xrange(iter_per_epoch):\n",
    "            batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "            batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "            # Run optimization op (backprop)\n",
    "            #print batch_ys.shape\n",
    "            sess.run([optimizer], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "                print \"Epoch %f, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(float(step)/iter_per_epoch, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "                Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "                Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "                Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "                Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "                Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch finished.\n",
    "        Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "        print \"Epoch %d, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(epoch+1, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "        Loss_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Loss_plt['train_y'].append(Train_Loss/iter_per_epoch)\n",
    "        Loss_plt['test_y'].append(Test_Loss/test_iter)\n",
    "        Acc_plt['x'].append(float(step)/iter_per_epoch)\n",
    "        Acc_plt['train_y'].append(Train_Acc/iter_per_epoch)\n",
    "        Acc_plt['test_y'].append(Test_Acc/test_iter)\n",
    "\n",
    "        duration = time.time()-begin\n",
    "        print \"Cost %f seconds\"%(duration)\n",
    "        \n",
    "        np.savez('outfile', loss=Loss_plt, acc=Acc_plt)\n",
    "        Vis=False\n",
    "        # Vis\n",
    "        if Vis == True:\n",
    "            index = np.random.randint(len(ims))\n",
    "            batch_ys = np.zeros((1))\n",
    "            batch_ys[0] = labels[index]\n",
    "            C1, C2 = sess.run([conv1, conv2], feed_dict={x: ims[index][np.newaxis,:], y:batch_ys, dropout: 0.0})\n",
    "            print C1.shape, C2.shape\n",
    "            for i in xrange(32):\n",
    "                plt.subplot(10,10,i+1),plt.imshow(C1[0,:,:,i],cmap='Greys_r'),plt.title('CONV1')\n",
    "            for i in xrange(64):\n",
    "                plt.subplot(10,10,100-i),plt.imshow(C2[0,:,:,i],cmap='Greys_r'),plt.title('CONV2')\n",
    "            plt.show()                                 \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.title('Loss.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.axis([0, training_epochs, 0, 5.0])\n",
    "    plt.plot(Loss_plt['x'], Loss_plt['train_y'], 'b-', label='training')\n",
    "    plt.plot(Loss_plt['x'], Loss_plt['test_y'], 'r-', label='testing')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy.')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('(%)')\n",
    "    plt.axis([0, training_epochs, 0, 1.0])\n",
    "    plt.plot(Acc_plt['x'], Acc_plt['train_y'], 'b-', label='training')\n",
    "    plt.plot(Acc_plt['x'], Acc_plt['test_y'], 'r-', label='testing')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'npzfiles/batchsize50.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7d81f7967cc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# See results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batchsize5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batchsize50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batchsize500'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d76b5d4b4ded>\u001b[0m in \u001b[0;36mplot_fcn\u001b[0;34m(types, tag)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mnpz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'npzfiles/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnpzfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnpzfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'npzfiles/batchsize50.npz'"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "types=['batchsize5', 'batchsize50', 'batchsize500']\n",
    "plot_fcn(types, tag='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Possible Results \n",
    "\n",
    "------After Random Initialization------<br>\n",
    "Training: loss=2.387872, acc=0.104417.\t\tTesting: loss=2.384974, acc=0.102800<br>\n",
    " 1.819188 seconds<br>\n",
    "------Start Training------<br>\n",
    "Epoch 0.000000, Training: loss=3.225684, acc=0.098717.\t\tTesting: loss=3.228740, acc=0.098000<br>\n",
    "Epoch 0.083333, Training: loss=2.297542, acc=0.125050.\t\tTesting: loss=2.298005, acc=0.127100<br>\n",
    "Epoch 0.166667, Training: loss=1.328221, acc=0.569167.\t\tTesting: loss=1.308034, acc=0.587500<br>\n",
    "Epoch 0.250000, Training: loss=0.686311, acc=0.780217.\t\tTesting: loss=0.664026, acc=0.796700<br>\n",
    "Epoch 0.333333, Training: loss=0.498580, acc=0.836733.\t\tTesting: loss=0.478692, acc=0.843300<br>\n",
    "Epoch 0.416667, Training: loss=0.363654, acc=0.888833.\t\tTesting: loss=0.351688, acc=0.892700<br>\n",
    "Epoch 0.500000, Training: loss=0.331383, acc=0.907383.\t\tTesting: loss=0.321058, acc=0.906400<br>\n",
    "Epoch 0.583333, Training: loss=0.252823, acc=0.924283.\t\tTesting: loss=0.241469, acc=0.925900<br>\n",
    "Epoch 0.666667, Training: loss=0.261203, acc=0.916517.\t\tTesting: loss=0.252525, acc=0.918900<br>\n",
    "Epoch 0.750000, Training: loss=0.212862, acc=0.932667.\t\tTesting: loss=0.203338, acc=0.936300<br>\n",
    "Epoch 0.833333, Training: loss=0.192696, acc=0.942000.\t\tTesting: loss=0.185617, acc=0.941900<br>\n",
    "Epoch 0.916667, Training: loss=0.176930, acc=0.945600.\t\tTesting: loss=0.169035, acc=0.946700<br>\n",
    "Epoch 1, Training: loss=0.185506, acc=0.941167.\t\tTesting: loss=0.174071, acc=0.945100<br>\n",
    "Cost 27.885424 seconds<br>\n",
    "Optimization Finished!<br>\n",
    "\n",
    "\n",
    "![](imgs/result_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorflow python API\n",
    "\n",
    "### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "\n",
    "Given an **input tensor of shape [batch, in_height, in_width, in_channels]** and a **filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]**, this op performs the following:\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "\n",
    "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "\n",
    "In detail, with the default NHWC format,\n",
    "\n",
    "output[b, i, j, k] = sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] * filter[di, dj, q, k]\n",
    "\n",
    "\n",
    "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].\n",
    "\n",
    "**Args:**\n",
    "\n",
    "> input: A Tensor. Must be one of the following types: half, float32, float64.\n",
    "\n",
    "> filter: A Tensor. Must have the same type as input.\n",
    "\n",
    "> strides: A list of ints. 1-D of length 4. The stride of the sliding window for each dimension of input. Must be in the same order as the dimension specified with format.\n",
    "\n",
    "> padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "use_cudnn_on_gpu: An optional bool. Defaults to True.\n",
    "\n",
    "> data_format: An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\". Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, in_height, in_width,\n",
    "in_channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, in_channels, in_height, in_width].\n",
    "\n",
    "> name: A name for the operation (optional).\n",
    "\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "> A Tensor. Has the same type as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
